{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Regression works well if the dependent variable has a Normal distribution. i.e. the labeled variable in the training set should be distributed normally\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\n",
    "\n",
    "*According to a survey in Forbes, data scientists spend 80% of their time on data preparation- with 60% of their time on cleaning and organizing the data*\n",
    "\n",
    "### What is a feature?\n",
    "- All machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns.\n",
    "\n",
    "Feature engineering efforts mainly have two goals:\n",
    "- Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n",
    "- Improving the performance of machine learning models.\n",
    "\n",
    "### Some of the most important techniques of feature engineering are as follows\n",
    "- Imputation: Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models. Some ways to handle missing values are\n",
    "    - Removing the row or column\n",
    "    - Filling the cell with 0\n",
    "    - Filling the cell with mean, median or mode\n",
    "    - Use the mean or median of the class the tuple(row) belongs to \n",
    "    - Use the most probable value - by using Regression or Decision Tree or Bayesan formalism\n",
    "- Handling Outliers\n",
    "    - Remove all outliers beyond +/- 2 Std\n",
    "    - Remove all outliers based on percentile < 5% and > 95%\n",
    "- Binning: The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance.\n",
    "    - It is mostly redundant for numerical values\n",
    "    - It is relevant mostly for categorical values like creating bins of (low, med, high)\n",
    "- Log Transform: It has several benefits usually for skewed data\n",
    "    - It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n",
    "    - Decreases the effect of outlier\n",
    "    - It normalizes the magnitude difference\n",
    "- One-Hot Encoding: One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\n",
    "    - Pandas get_dummies() function creates separate columns for these categorical values\n",
    "- Grouping Operations: Aggregate functions are used to group categorical and numerical values for rows. Group-by functionality is leveraged.\n",
    "- Feature Split: Features are split to extract valuable information from string features.\n",
    "- Scaling: Min-max or standard scalers are used to normalize features across the data-frame\n",
    "- Extracting Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler vs Max-Min Scaler\n",
    "- Standard\n",
    "    - Standar Scaler uses z-score scaling\n",
    "    - It usually doesn't do any harm to any distribution. \n",
    "- Max Min\n",
    "    - Max-Min Scaler scales between (min, max) value provided, usually (0,1)\n",
    "    - It suppresses the Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "- Regression Loss Functions\n",
    "   - Mean Squared Error Loss: Default loss funtion for most regression problesm\n",
    "   - Mean Squared Logarithmic Error Loss: There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error.\n",
    "   - Mean Absolute Error Loss: The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more *robust to outliers*. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
    "- Binary Classification Loss Functions:\n",
    "Cross-entropy is the default loss function to use for binary classification problems.\n",
    "   - Binary Cross-Entropy\n",
    "   - Hinge Loss\n",
    "   - Squared Hinge Loss\n",
    "- Multi-Class Classification Loss Functions: Keras function is \"categorical_crossentropy\" and the activation function used is Softmax\n",
    "   - Multi-Class Cross-Entropy Loss\n",
    "   - Sparse Multiclass Cross-Entropy Loss\n",
    "   - Kullback Leibler Divergence Loss: Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution. As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to evaluate models\n",
    "\n",
    "- **False negative (Type-2)** is the most dangerous type of error. It should be avoided at any cost. For example it predicts that the patient is healthy when actually the patient has some ailments\n",
    "- **False positive (Type-1)** is not as dangerous as Type-1 but should be avoided. For example it predicts a patient is unhealthy when actually he is healthy.\n",
    "- **True positive**: actual is True and predicted is true. (valid)\n",
    "- **True negative**: actual is False and predicted is False. (valid)\n",
    "\n",
    "\n",
    "Popular types of metrics are as follows\n",
    "\n",
    "- Classification Accuracy: It is defined as the (Total correct prediction)/(Total No. of predictions)\n",
    "\n",
    "- Logarithmic Loss\n",
    "    - $ LogarithmicLoss = \\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^M y_{ij}.log(p_{ij})$\n",
    "    - It is good for multi-class classification\n",
    "- Confusion Matrix\n",
    "- Area under Curve\n",
    "- F1 Score (Precision and Recall):\n",
    "    - F1 Score is the Harmonic Mean between precision and recall\n",
    "    - $F1 = 2 * \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}$\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "- Each neuron value needs to be minimised, as original input data can be very diverse, and perhaps out of proportion. Before forward feeding further, n5,n6,n7 must be activated. In simple terms, there are a series of functions you could use that act as a linear or non-linear threshold on values arriving at a neuron\n",
    "-  A Neural Network without Activation function would simply be a **Linear regression Model**, which has limited power and does not performs good most of the times. \n",
    "- **Non-linear functions** are those which have degree more than one and they have a curvature when we plot a Non-Linear function. Now we need a Neural Network Model to learn and represent almost anything and any arbitrary complex function which maps inputs to outputs\n",
    "-  Neural-Networks are considered **Universal Function Approximators**. It means that they can compute and learn any function at all. Almost any process we can think of can be represented as a functional computation in Neural Networks.\n",
    "- Also another important feature of a Activation function is that it should be **differentiable**. We need it to be this way so as to perform backpropogation optimization strategy while propogating backwards in the network to compute gradients of Error(loss) with respect to Weights and then accordingly optimize weights using Gradient descend or any other Optimization technique to reduce Error.\n",
    "- Types of Activation Functions\n",
    "    - Linear: This is as simple as Linear Regression model\n",
    "    - Sigmoid: Output between (0,1). Issue is **Vanishing Gradient problem**. Used mostly at output layer\n",
    "    - Tanh: Output between (-1,1)\n",
    "    - RELU: Output is max(0,n). It can be used only in hidden layers. Issue is it can cause **Dead neurons**. The neuron will become 0 and will never activate again.\n",
    "    - Leaky RELU: Fixes the problem of RELU. It introduces a small slope(0.1) to keep the updates alive.\n",
    "    - Softmax: Mostly used for mutli-class classification problem at the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic vs Batch vs Mini-batch\n",
    "\n",
    "- Stochastic Gradient Descent: A smaller minibatch size induces more noise in their error calculations and often more useful in preventing the training process from stopping at **local minima**.\n",
    "\n",
    "- Batch Gradient Descent: A larger minibatch size allows computational boosts that utilizes matrix multiplication in the training calculations but that comes at the expense of needing more memory for the training process.\n",
    "\n",
    "- Mini-batch Gradient Descent: Good value for minibatch size= 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters\n",
    "- Hyperparameters are varaibles that we need to set before applying a learning algorithm to a dataset.\n",
    "- The challenge with hyperparameters is that there are no magic number that works everywhere. The best numbers depend on each task and each dataset\n",
    "\n",
    "## Types of Hyper parameters\n",
    "Hyper parameters are divided into 2 types\n",
    "- Optimizer hyperparameters\n",
    "    - Learning Rate\n",
    "    - Mini batch size: mini batch = 1 is called as SGD. Mini-batch= n is called GD where the entire dataset is feed in a single pass and then loss is calculated and back propagated. The third method is to have a mini-batch size of 2,4,8,16,32,64 etc. Good value is **32**\n",
    "    - Number of epochs: Intuitive manual way is to have the model train for as many number of iterations as long as validation error keeps decreasing.\n",
    "    - \n",
    "- Model hyperparameters\n",
    "    - Number of hidden units\n",
    "    - First hidden unit\n",
    "    - Number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

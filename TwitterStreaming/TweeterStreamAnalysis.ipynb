{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common libraries\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark libraries\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import  StreamingContext\n",
    "from pyspark.sql import  SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6877be15333e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import Kafka libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkafka\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaConsumer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mkafka_hostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'35.239.57.13'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kafka'"
     ]
    }
   ],
   "source": [
    "# Kafka Server configuration\n",
    "\n",
    "# Import Kafka libraries\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "\n",
    "kafka_hostname = '35.239.57.13'\n",
    "kafka_port = '9092'\n",
    "kafka_bootstrap_server = kafka_hostname + ':' + kafka_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register to Kafka as a Producer\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[kafka_bootstrap_server], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publishes the message to Kafka Topic\n",
    "\n",
    "def publishToKafka(topic_name, data):\n",
    "    producer = connect_kafka_producer()\n",
    "    text_to_send = bytes(data, encoding='utf-8')\n",
    "    try:\n",
    "        producer.send(topic_name,value=text_to_send)\n",
    "        producer.flush()\n",
    "        print(f\"Message delivered to Kafka Topic - {topic_name}\")\n",
    "    except Exception as ex:\n",
    "        #print(\"Message sending failed!!\")\n",
    "        print(ex)\n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 1\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def sentimentCategoryBlob(score):\n",
    "    if score == 0.0:\n",
    "        category = \"neutral\"\n",
    "    elif score > 0.0:\n",
    "        category = \"positive\"\n",
    "    elif score < 0.0:\n",
    "        category = \"negative\"\n",
    "        \n",
    "    return category\n",
    "\n",
    "# Cleans the tweet Text\n",
    "def clean_tweet(text):\n",
    "    \"\"\"\n",
    "    Removes the junk characters and tweet names\n",
    "    \"\"\"\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \",text).split())\n",
    "\n",
    "# Applies Blob Algorithm to find sentiments to each texts\n",
    "def blobSentimentAnalysis(text):\n",
    "    \n",
    "    text = clean_tweet(text)\n",
    "    analysis = TextBlob(text)\n",
    "    \n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "def findBlobSentiment(text):\n",
    "    sentiment = blobSentimentAnalysis(str(text))\n",
    "    #print(sentiment)\n",
    "    category = sentimentCategoryBlob(sentiment)\n",
    "    return category\n",
    "\n",
    "def tweetsPerSentiment(dic) :\n",
    "    topic_name = \"blob_sentiments\"\n",
    "    senti_map = {\"positive\":0, \"negative\":0, \"neutral\": 0}\n",
    "    for key in dic.keys():\n",
    "        senti_map[key] = dic[key]\n",
    "    #print(dic)\n",
    "    print(senti_map)\n",
    "    if dic:\n",
    "        publishToKafka(topic_name, json.dumps(senti_map))\n",
    "        #pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 2\n",
    "\n",
    "import re\n",
    "\n",
    "def electionMap(tweet):\n",
    "    parties_list = list()\n",
    "    if tweet: \n",
    "        tweet = tweet.lower()\n",
    "        if re.search(r\"shivsena|janata|jdu|bjp|modi|namo|chowkidar|shah|nda\", tweet):\n",
    "            parties_list.insert(0,\"BJP\")\n",
    "        if re.search(r\"indiancongress|raga|gandhi|inc|sonia|congress|rahul|priyanka|gatbandhan|nyay\", tweet):\n",
    "            parties_list.insert(0,\"INC\")\n",
    "        if re.search(r\"aap|kejri|arvind\", tweet):\n",
    "            parties_list.insert(0,\"AAP\") \n",
    "        if re.search(r\"mamata|cpi|kanhaiya|bsp|samajwadi|tmc|trinamool|dmk|mns|bjd|samajwadi|yadav|gatbandhan|naidu|kalyan\", tweet):\n",
    "            parties_list.insert(0,\"Others\")\n",
    "    return parties_list\n",
    "\n",
    "def tweetsPerParty(dic):\n",
    "    topic_name = \"election_parties\"\n",
    "    election_map = {\"BJP\": 0, \"INC\": 0, \"AAP\": 0, \"Others\": 0}\n",
    "    for key in dic.keys():\n",
    "        election_map[key] = dic[key]\n",
    "    print(election_map)\n",
    "    if dic:\n",
    "        publishToKafka(topic_name, json.dumps(election_map))\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 3\n",
    "\n",
    "# Find top 6 hashtags\n",
    "def topTrendingHashTags(records):\n",
    "    topic_name = \"trending_hashtags\"\n",
    "    top_k_tweets = {}\n",
    "    if records:\n",
    "        top_k_tweets = dict(records)\n",
    "    print(top_k_tweets)\n",
    "    if records:\n",
    "        publishToKafka(topic_name, json.dumps(top_k_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a tweet stream to JSON\n",
    "def convert_tweet(tweet):\n",
    "    if not tweet:\n",
    "        tweet = \"{}\"\n",
    "    return json.loads(tweet)\n",
    "\n",
    "# Extract Tweet Text from tweet JSON\n",
    "def extractTweetText(tweet):\n",
    "    try:\n",
    "        text = tweet['text']\n",
    "        \n",
    "    except KeyError as ex:\n",
    "        print(\"No key attribute - text in the JSON\")\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "# Extract Hashtags from a tweet JSON\n",
    "def extractTweetHashtags(tweet):\n",
    "    try:\n",
    "        # hashtags is a list\n",
    "        hashtags = tweet['entities']['hashtags']\n",
    "        \n",
    "    except KeyError as ex:\n",
    "        print(\"No key attribute - entities:hashtags in the JSON\")\n",
    "        hashtags = \"\"\n",
    "        \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ae07c1c645ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msliding_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mspc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mspc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venvpy3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venvpy3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/envs/venvpy3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venvpy3/lib/python3.6/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \"\"\"\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_launch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venvpy3/lib/python3.6/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36m_launch_gateway\u001b[0;34m(conf, insecure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 3:\n",
    "        print(f\"Usage: python <file_name> <netcat_host_name> <netcat_port>\")\n",
    "        exit(-1)\n",
    "\n",
    "    nc_host_name = \"localhost\"\n",
    "    nc_port = 9999\n",
    "    \n",
    "    # batch interval in seconds\n",
    "    batch_interval = 1\n",
    "    window_length = 15 * batch_interval\n",
    "    sliding_window = 1 * batch_interval\n",
    "\n",
    "    spc = SparkContext.getOrCreate()\n",
    "    spc.setLogLevel(\"ERROR\")\n",
    "    stc = StreamingContext(spc, batch_interval)\n",
    "    stc.checkpoint(\"/tmp/checkpoint\")\n",
    "\n",
    "    tweetsDStream = stc.socketTextStream(nc_host_name, nc_port)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Converts tweet stream to json object\n",
    "        tstream = tweetsDStream.map(lambda tweet: convert_tweet(tweet))\n",
    "        #tstream = tstream.filter(lambda tweet: filterOnLang(tweet))\n",
    " \n",
    "        # Extracts the text from tweet json\n",
    "        textstream = tstream.map(lambda tweet: extractTweetText(tweet))\n",
    "        \n",
    "        # Counts the number of records in the stream batch\n",
    "        tstream.count().map(lambda x:'Tweets in this batch: %s' % x).pprint()\n",
    "        \n",
    "        # Counts the number of records in the streaming window\n",
    "        tstream.countByWindow(window_length,sliding_window).map(lambda x: \"Tweets in this 1 minute window: %s\" %x).pprint()\n",
    "        \n",
    "        ################## Use Case 2: Tweets per party ####################\n",
    "        \n",
    "        # Returns a map of election parties and their no. of tweets\n",
    "        election = textstream.flatMap(lambda tweet: electionMap(tweet))\n",
    "        election.countByValueAndWindow(window_length,sliding_window).foreachRDD(lambda rdd: tweetsPerParty(dict(rdd.collect())))\n",
    "      \n",
    "        ################## Use Case 3: Trending Hashtags ####################\n",
    "        \n",
    "        # Find the dictionary of top 6 Trending Hashtags in each tweets\n",
    "        hashtagstream = tstream.map(lambda tweet: extractTweetHashtags(tweet))\n",
    "        hashtagstream = hashtagstream.flatMap(lambda hashtags: [ hashtag['text'] for hashtag in hashtags if hashtag ])\n",
    "        hashtagstream.countByValueAndWindow(window_length,sliding_window).transform(lambda rdd: rdd.sortBy(lambda x: -x[1])).foreachRDD(lambda rdd: topTrendingHashTags(rdd.take(6)))\n",
    "    \n",
    "        ################## Use Case 1: Sentiment Analysis ####################\n",
    "        \n",
    "        # Finds the Sentiments of the tweets using Blob based algorithm\n",
    "        sentimentstream = textstream.map(lambda text: findBlobSentiment(text))\n",
    "        sentimentstream.countByValueAndWindow(window_length,sliding_window).foreachRDD(lambda rdd: tweetsPerSentiment(dict(rdd.collect())))\n",
    "        \n",
    "        stc.start()\n",
    "        stc.awaitTermination()\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpy3",
   "language": "python",
   "name": "venvpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
